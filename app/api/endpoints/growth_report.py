from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field, AliasChoices, ConfigDict
from typing import List, Dict, Any, Optional
import logging
import json

logger = logging.getLogger("dinory.growth_report")
if not logger.handlers:
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("[GROWTH_REPORT] %(asctime)s || %(levelname)s || %(message)s"))
    logger.addHandler(h)
logger.setLevel(logging.INFO)

router = APIRouter(tags=["ai"])

try:
    from app.services.llm.openai_service import OpenAIService
except Exception as e:
    logger.warning(f"OpenAIService import ì‹¤íŒ¨: {e}")
    OpenAIService = None

# ================== ëª¨ë¸ ================== 

class GrowthReportRequest(BaseModel):
    model_config = ConfigDict(extra='ignore', populate_by_name=True)

    beforeAbilities: Dict[str, float] = Field(validation_alias=AliasChoices('beforeAbilities', 'before_abilities'))
    afterAbilities: Dict[str, float] = Field(validation_alias=AliasChoices('afterAbilities', 'after_abilities'))
    strengths: Optional[List[Dict[str, Any]]] = []
    growthAreas: Optional[List[Dict[str, Any]]] = Field(default_factory=list, validation_alias=AliasChoices('growthAreas', 'growth_areas'))
    totalStories: int = Field(default=0, validation_alias=AliasChoices('totalStories', 'total_stories'))
    period: str = "month"

# ================== ì—”ë“œí¬ì¸íŠ¸ ==================     

@router.post("/generate-growth-evaluation")
async def generate_growth_evaluation(req: GrowthReportRequest):
    """AI ì¢…í•© í‰ê°€ ìƒì„±"""
    logger.info(f"ì„±ì¥ í‰ê°€ ìƒì„± ìš”ì²­: period={req.period}, totalStories={req.totalStories}")
    try:
        if OpenAIService:
            llm = OpenAIService()

            # Before/After ëŠ¥ë ¥ì¹˜ ë¹„êµ
            before_text = "\n".join([f"- {k}: {v:.0f}ì " for k, v in req.beforeAbilities.items()])
            after_text = "\n".join([f"- {k}: {v:.0f}ì " for k, v in req.afterAbilities.items()])

            # ëŠ¥ë ¥ì¹˜ ë³€í™” ê³„ì‚°
            changes = []
            for ability, after_score in req.afterAbilities.items():
                before_score = req.beforeAbilities.get(ability, 0)
                change = after_score - before_score
                if abs(change) > 5: # 5ì  ì´ìƒ ë³€í™”ë§Œ
                    changes.append(f"{ability}: {change:+.0f}ì ")

            changes_text = ", ".join(changes) if changes else "ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì "

            # ê°•ì  ì˜ì—­ (ì˜ˆì‹œ í¬í•¨)
            strengths_detail = []
            for s in req.strengths[:3]:  # ìƒìœ„ 3ê°œ
                area = s.get("area", "")
                score = s.get("score", 0)
                examples = s.get("examples", [])
                examples_text = ", ".join(examples[:2]) if examples else ""
                strengths_detail.append(f"{area} ({score:.0f}ì ): {examples_text}")
            strengths_text = "\n- ".join(strengths_detail) if strengths_detail else "ì—†ìŒ"

            # ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ (ì˜ˆì‹œ í¬í•¨)
            growth_detail = []
            for g in req.growthAreas[:3]:  # ìƒìœ„ 3ê°œ
                area = g.get("area", "")
                score = g.get("score", 0)
                examples = g.get("examples", [])
                examples_text = ", ".join(examples[:2]) if examples else ""
                growth_detail.append(f"{area} ({score:.0f}ì ): {examples_text}")
            growth_areas_text = "\n- ".join(growth_detail) if growth_detail else "ì—†ìŒ"

            period_map = {"month": "í•œ ë‹¬", "quarter": "3ê°œì›”", "halfyear": "6ê°œì›”"}
            period_text = period_map.get(req.period, "í•œ ë‹¬")

            prompt = f"""

ë‹¹ì‹ ì€ ì•„ë™ì‹¬ë¦¬ì „ë¬¸ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì•„ì´ì˜ {period_text}ê°„ ì„±ì¥ ë¦¬í¬íŠ¸ë¥¼ ìœ„í•œ ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” ê°ê´€ì ì¸ ì¢…í•© í‰ê°€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ê¸°ë³¸ ì •ë³´**:
- ì™„ë£Œí•œ ë™í™”: {req.totalStories}ê°œ
- ê¸°ê°„: {period_text}

**ì´ì „ ëŠ¥ë ¥ì¹˜**:
{before_text}

**í˜„ì¬ ëŠ¥ë ¥ì¹˜**:
{after_text}

**ì£¼ìš” ë³€í™”**: {changes_text}

**ê°•ì  ì˜ì—­ (êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨)**:
- {strengths_text}

**ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ (êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨)**:
- {growth_areas_text}

ì¡°ê±´:
1. **ìµœì†Œ 1000ì ì´ìƒ ì‘ì„± (ë§¤ìš° ì¤‘ìš”!)** - 3-4ê°œ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±
2. ê° ëŠ¥ë ¥ì¹˜ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª… (ì˜ˆ: ìš©ê¸° â†’ ìƒˆë¡œìš´ ë„ì „ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•ŠëŠ” ë§ˆìŒ)
3. **ê°•ì  ì˜ì—­ì˜ êµ¬ì²´ì  ì˜ˆì‹œë¥¼ í™œìš©**í•˜ì—¬ ì•„ì´ì˜ ì‹¤ì œ í–‰ë™ì„ ì–¸ê¸‰í•˜ê³  ì¹­ì°¬
4. ì™„ë£Œí•œ ë™í™” ê°œìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ì´ì˜ ë…¸ë ¥ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ì •
5. ê¸ì •ì ì´ê³  ì„±ì¥ ê°€ëŠ¥ì„±ì— ì´ˆì ì„ ë§ì¶˜ ê²©ë ¤
6. ë¶€ëª¨ê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ í•œêµ­ì–´
7. í‰ê°€ë¬¸ë§Œ ì‘ì„± (ì œëª©, ì¸ì‚¬ë§, "~ë“œë¦½ë‹ˆë‹¤" ê°™ì€ ê²°ì–´ ì œì™¸)
8. ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ ì•„ì´ì˜ ì ì¬ë ¥ê³¼ ê°€ëŠ¥ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ **í’ë¶€í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ** ì‘ì„±
9. ê° ë¬¸ë‹¨ ì‚¬ì´ì— ë¹ˆ ì¤„ì„ ë„£ì–´ ê°€ë…ì„± ìˆê²Œ ì‘ì„±
10. **ê° ë¬¸ë‹¨ë§ˆë‹¤ ìµœì†Œ 250ì ì´ìƒ ì‘ì„±í•˜ì—¬ ì „ì²´ 1000ì ì´ìƒ ë‹¬ì„±**
11. ì˜ë¬¸ì€ ë°˜ë“œì‹œ ì œì™¸!

**êµ¬ì¡° (ê° ë¬¸ë‹¨ ìµœì†Œ 250ì ì´ìƒ)**:
- 1ë¬¸ë‹¨: ì „ì²´ì ì¸ ì„±ì¥ ê°œìš”ì™€ ì™„ë£Œí•œ ë™í™”ì— ëŒ€í•œ ì¹­ì°¬ (ì•„ì´ì˜ ë…¸ë ¥ê³¼ ì„±ì¥ì„ í’ë¶€í•˜ê²Œ í‘œí˜„)
- 2ë¬¸ë‹¨: ê°•ì  ì˜ì—­ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ê²©ë ¤ (ì˜ˆì‹œì— ë‚˜ì˜¨ êµ¬ì²´ì  í–‰ë™ì„ ì–¸ê¸‰í•˜ë©° ì¹­ì°¬)
- 3ë¬¸ë‹¨: ì£¼ìš” ë³€í™”ì™€ ë°œì „ ë‚´ìš© (ëŠ¥ë ¥ì¹˜ ë³€í™”ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…)
- 4ë¬¸ë‹¨: ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ê³¼ ì•ìœ¼ë¡œì˜ ê¸°ëŒ€ê° (ë¶€ëª¨ì™€ í•¨ê»˜ í•  ìˆ˜ ìˆëŠ” ë°©í–¥ ì œì‹œ)
"""
            response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=2500
            )

            evaluation = response.choices[0].message.content.strip()
            logger.info(f"AI í‰ê°€ ìƒì„± ì™„ë£Œ: {len(evaluation)}ì")

            return {"evaluation": evaluation}
        
        else:
            # OpenAI ì„œë¹„ìŠ¤ ì—†ì„ ë•Œ í’€ë°±
            logger.info("OpenAIService ì—†ìŒ -> í…œí”Œë¦¿ ì‚¬ìš©")
            period_map = {"month": "í•œ ë‹¬", "quarter": "3ê°œì›”", "halfyear": "6ê°œì›”"}
            fallback = f"ì´ë²ˆ {period_map.get(req.period, 'í•œ ë‹¬')}ê°„ ì•„ì´ëŠ” {req.totalStories}ê°œì˜ ë™í™”ë¥¼ ì™„ë£Œí•˜ì—¬ ê¸ì •ì ì¸ ì„±ì¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤."
            if req.strengths:
                fallback += f" íŠ¹íˆ {req.strengths[0].get('area', '')} ì˜ì—­ì—ì„œ ë›°ì–´ë‚œ ëª¨ìŠµì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."
            return {"evaluation": fallback}
            
    except Exception as e:
        logger.exception("generate-growth-evaluation ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))



@router.post("/generate-growth-recommendations")
async def generate_growth_recommendations(req: GrowthReportRequest):
    """AI ê¸°ë°˜ ì¶”ì²œ í™œë™ ìƒì„±"""
    logger.info(f"ì¶”ì²œ í™œë™ ìƒì„± ìš”ì²­: growthAreas={len(req.growthAreas)}ê°œ")
    try:
        if OpenAIService:
            llm = OpenAIService()
            
            # ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ ì •ë³´
            if not req.growthAreas:
                logger.warning("ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ ë°ì´í„° ì—†ìŒ")
                return {"recommendations": []}
            
            growth_areas_info = "\n".join([
                f"- {g.get('area', '')}: {g.get('score', 0)}ì  ({g.get('description', '')})"
                for g in req.growthAreas[:3]  # ìµœëŒ€ 3ê°œ
            ])
            
            prompt = f"""
ì•„ì´ì˜ ì„±ì¥ì„ ìœ„í•œ ë§ì¶¤ í™œë™ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ì„±ì¥ ê°€ëŠ¥ ì˜ì—­**:
{growth_areas_info}

ìœ„ ì˜ì—­ë“¤ì„ ê³ ë ¤í•˜ì—¬ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ìˆœì„œë¡œ 3ê°€ì§€ í™œë™ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):
{{
  "recommendations": [
    {{
      "priority": 1,
      "activity": "í™œë™ ì´ë¦„ (10ì ì´ë‚´)",
      "description": "êµ¬ì²´ì ì¸ í™œë™ ì„¤ëª… (40ì ì´ë‚´)",
      "targetArea": "íƒ€ê²Ÿ ëŠ¥ë ¥ì¹˜"
    }},
    {{
      "priority": 2,
      "activity": "í™œë™ ì´ë¦„",
      "description": "í™œë™ ì„¤ëª…",
      "targetArea": "íƒ€ê²Ÿ ëŠ¥ë ¥ì¹˜"
    }},
    {{
      "priority": 3,
      "activity": "í™œë™ ì´ë¦„",
      "description": "í™œë™ ì„¤ëª…",
      "targetArea": "íƒ€ê²Ÿ ëŠ¥ë ¥ì¹˜"
    }}
  ]
}}

ì¡°ê±´:
1. ì•„ì´ê°€ ì‹¤ì œë¡œ í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì´ê³  ì¬ë¯¸ìˆëŠ” í™œë™
2. ë¶€ëª¨ê°€ í•¨ê»˜ í•  ìˆ˜ ìˆëŠ” í™œë™
3. ì¼ìƒì—ì„œ ì‰½ê²Œ ì‹¤ì²œ ê°€ëŠ¥
4. ì •í™•í•œ JSON í˜•ì‹ (ì‰¼í‘œ, ê´„í˜¸ ì£¼ì˜)
"""
            
            response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.7
            )
            
            result = json.loads(response.choices[0].message.content)
            recommendations = result.get("recommendations", [])
            
            logger.info(f"ì¶”ì²œ í™œë™ ìƒì„± ì™„ë£Œ: {len(recommendations)}ê°œ")
            return {"recommendations": recommendations}
            
        else:
            # í´ë°±
            logger.info("OpenAIService ì—†ìŒ â†’ ê¸°ë³¸ ì¶”ì²œ ì‚¬ìš©")
            fallback_recs = []
            for i, area in enumerate(req.growthAreas[:3]):
                fallback_recs.append({
                    "priority": i + 1,
                    "activity": f"{area.get('area', '')} í–¥ìƒ í™œë™",
                    "description": f"ì•„ì´ì™€ í•¨ê»˜ {area.get('area', '')} ëŠ¥ë ¥ì„ í‚¤ìš°ëŠ” í™œë™ì„ í•´ë³´ì„¸ìš”.",
                    "targetArea": area.get('area', '')
                })
            return {"recommendations": fallback_recs}
            
    except Exception as e:
        logger.exception("generate-growth-recommendations ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate-growth-area-descriptions")
async def generate_growth_area_descriptions(req: GrowthReportRequest):
    """ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ì¶”ì²œ ìƒì„±"""
    logger.info(f"ì„±ì¥ ì˜ì—­ ì„¤ëª… ìƒì„± ìš”ì²­: {len(req.growthAreas)}ê°œ")
    try:
        if not OpenAIService or not req.growthAreas:
            return {"descriptions": []}

        llm = OpenAIService()
        results = []

        for area_info in req.growthAreas[:3]:
            area_name = area_info.get("area", "")
            score = area_info.get("score", 0)

            prompt = f"""
ì•„ì´ì˜ {area_name} ëŠ¥ë ¥(í˜„ì¬ {score}ì )ì„ ë°œì „ì‹œí‚¤ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ì¶”ì²œì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "description": "{area_name}ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì™œ ì¤‘ìš”í•œì§€ 30ì ì´ë‚´ë¡œ",
  "recommendation": "ë¶€ëª¨ê°€ ì•„ì´ì™€ í•¨ê»˜ í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í™œë™ 1ê°€ì§€ë¥¼ 40ì ì´ë‚´ë¡œ"
}}

ì¡°ê±´:
1. ì•„ë™ ë°œë‹¬ ì‹¬ë¦¬í•™ ê´€ì ì—ì„œ ì‘ì„±
2. ì‹¤ìƒí™œì—ì„œ ë°”ë¡œ ì‹¤ì²œ ê°€ëŠ¥í•œ ë‚´ìš©
3. "~í•´ë³´ì„¸ìš”", "~í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤" ë“± ë¶€ë“œëŸ¬ìš´ ì–´ì¡°
"""

            try:
                response = llm.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.7
                )

                result = json.loads(response.choices[0].message.content)
                results.append({
                    "area": area_name,
                    "score": score,
                    "description": result.get("description", f"{area_name} ì˜ì—­ì„ ë” ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
                    "recommendation": result.get("recommendation", f"{area_name} ê´€ë ¨ ë™í™”ë¥¼ í•¨ê»˜ ì½ì–´ë³´ì„¸ìš”.")
                })
            except Exception as e:
                logger.warning(f"{area_name} ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
                results.append({
                    "area": area_name,
                    "score": score,
                    "description": f"{area_name} ì˜ì—­ì„ ë” ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "recommendation": f"{area_name} ê´€ë ¨ ë™í™”ë¥¼ í•¨ê»˜ ì½ì–´ë³´ì„¸ìš”."
                })

        logger.info(f"ì„±ì¥ ì˜ì—­ ì„¤ëª… ìƒì„± ì™„ë£Œ: {len(results)}ê°œ")
        return {"descriptions": results}

    except Exception as e:
        logger.exception("generate-growth-area-descriptions ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate-milestones")
async def generate_milestones(req: GrowthReportRequest):
    """AI ê¸°ë°˜ ë§ˆì¼ìŠ¤í†¤ ìƒì„±"""
    logger.info(f"ë§ˆì¼ìŠ¤í†¤ ìƒì„± ìš”ì²­: totalStories={req.totalStories}")
    try:
        if not OpenAIService:
            return {"milestones": []}

        llm = OpenAIService()
        milestones = []

        # 1. ë™í™” ì™„ë£Œ ë§ˆì¼ìŠ¤í†¤
        if req.totalStories >= 5:
            prompt = f"""
ì•„ì´ê°€ {req.totalStories}ê°œì˜ ë™í™”ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì´ ì„±ì·¨ë¥¼ ì¶•í•˜í•˜ëŠ” ë§ˆì¼ìŠ¤í†¤ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "achievement": "ì¶•í•˜ ë¬¸êµ¬ (20ì ì´ë‚´, êµ¬ì²´ì ì´ê³  ê°ë™ì ìœ¼ë¡œ)"
}}

ì¡°ê±´:
1. ì•„ì´ì˜ ë…¸ë ¥ê³¼ ê¾¸ì¤€í•¨ì„ ê°•ì¡°
2. ê¸ì •ì ì´ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°
3. ìˆ«ìë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
"""
            try:
                response = llm.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.7
                )
                result = json.loads(response.choices[0].message.content)
                milestones.append({
                    "achievement": result.get("achievement", f"{req.totalStories}ê°œì˜ ë™í™”ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤"),
                    "date": None  # Spring Bootì—ì„œ ì„¤ì •
                })
            except Exception as e:
                logger.warning(f"ë™í™” ì™„ë£Œ ë§ˆì¼ìŠ¤í†¤ ìƒì„± ì‹¤íŒ¨: {e}")

        # 2. ë†’ì€ ëŠ¥ë ¥ì¹˜ ë§ˆì¼ìŠ¤í†¤
        for ability, score in req.afterAbilities.items():
            if score >= 75:
                prompt = f"""
ì•„ì´ê°€ {ability} ëŠ¥ë ¥ì—ì„œ {score:.0f}ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ ì„±ì·¨ë¥¼ ì¶•í•˜í•˜ëŠ” ë§ˆì¼ìŠ¤í†¤ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "achievement": "ì¶•í•˜ ë¬¸êµ¬ (25ì ì´ë‚´, {ability}ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ)"
}}

ì¡°ê±´:
1. ëŠ¥ë ¥ì¹˜ ì´ë¦„ì„ ì•„ì´ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…
2. ì ìˆ˜ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
3. ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°
"""
                try:
                    response = llm.client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": prompt}],
                        response_format={"type": "json_object"},
                        temperature=0.7
                    )
                    result = json.loads(response.choices[0].message.content)
                    milestones.append({
                        "achievement": result.get("achievement", f"{ability} ëŠ¥ë ¥ {score:.0f}ì  ë‹¬ì„±"),
                        "date": None
                    })
                except Exception as e:
                    logger.warning(f"{ability} ë§ˆì¼ìŠ¤í†¤ ìƒì„± ì‹¤íŒ¨: {e}")

        logger.info(f"ë§ˆì¼ìŠ¤í†¤ ìƒì„± ì™„ë£Œ: {len(milestones)}ê°œ")
        return {"milestones": milestones}

    except Exception as e:
        logger.exception("generate-milestones ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate-strength-descriptions")
async def generate_strength_descriptions(req: GrowthReportRequest):
    """ê°•ì  ì˜ì—­ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª… ìƒì„±"""
    logger.info(f"ê°•ì  ì„¤ëª… ìƒì„± ìš”ì²­: {len(req.strengths)}ê°œ")
    try:
        if not OpenAIService or not req.strengths:
            return {"descriptions": []}

        llm = OpenAIService()
        results = []

        for strength_info in req.strengths[:3]:
            area_name = strength_info.get("area", "")
            score = strength_info.get("score", 0)
            examples = strength_info.get("examples", [])  # ë°°ì—´ë¡œ ë°›ê¸°

            prompt = f"""
ì•„ì´ê°€ {area_name} ëŠ¥ë ¥ì—ì„œ {score}ì ì„ ê¸°ë¡í•˜ë©° ë›°ì–´ë‚œ ëª¨ìŠµì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
{f"ì˜ˆì‹œ: {', '.join(examples)}" if examples else ""}

ì´ ê°•ì ì„ ë¶€ëª¨ì—ê²Œ ë³´ê³ í•˜ëŠ” ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "description": "{area_name}ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì•„ì´ì˜ ê°•ì ì„ 3ì¸ì¹­ìœ¼ë¡œ ì„¤ëª… (ì˜ˆ: ì•„ì´ëŠ”, ì•„ì´ì˜) 40ì ì´ë‚´"
}}

ì¡°ê±´:
1. ë¶€ëª¨ì—ê²Œ ë³´ê³ í•˜ëŠ” í˜•ì‹ (3ì¸ì¹­: ì•„ì´ëŠ”, ì•„ì´ì˜)
2. ì•„ì´ì˜ ì„±ì·¨ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì¹­ì°¬
3. {area_name} ëŠ¥ë ¥ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
4. ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°
"""

            try:
                response = llm.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.7
                )

                result = json.loads(response.choices[0].message.content)
                results.append({
                    "area": area_name,
                    "score": score,
                    "description": result.get("description", f"{area_name} ì˜ì—­ì—ì„œ ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."),
                    "examples": examples  # ë°°ì—´ë¡œ ë°˜í™˜
                })
            except Exception as e:
                logger.warning(f"{area_name} ê°•ì  ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
                results.append({
                    "area": area_name,
                    "score": score,
                    "description": f"{area_name} ì˜ì—­ì—ì„œ ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
                    "examples": examples  # ë°°ì—´ë¡œ ë°˜í™˜
                })

        logger.info(f"ê°•ì  ì„¤ëª… ìƒì„± ì™„ë£Œ: {len(results)}ê°œ")
        return {"descriptions": results}

    except Exception as e:
        logger.exception("generate-strength-descriptions ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate-example-description")
async def generate_example_description(request: Dict[str, Any]):
    """ê°•ì  ì˜ˆì‹œë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜"""
    logger.info("ì˜ˆì‹œ ì„¤ëª… ìƒì„± ìš”ì²­")
    try:
        story_title = request.get("storyTitle", "")
        choice_text = request.get("choiceText", "")
        ability = request.get("ability", "")

        if not OpenAIService or not story_title or not choice_text:
            return {"example": f"'{story_title}'ì—ì„œ '{choice_text}'ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤."}

        llm = OpenAIService()

        prompt = f"""
ì•„ì´ê°€ '{story_title}'ë¼ëŠ” ë™í™”ì—ì„œ '{choice_text}'ë¼ëŠ” ì„ íƒì„ í–ˆìŠµë‹ˆë‹¤.
ì´ ì„ íƒì´ {ability} ëŠ¥ë ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ë¶€ëª¨ì—ê²Œ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "example": "ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ì„¤ëª… (30ì ì´ë‚´)"
}}

ì¡°ê±´:
1. ë™í™” ì œëª©ê³¼ ì„ íƒ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
2. {ability} ëŠ¥ë ¥ê³¼ ì—°ê²°í•˜ì—¬ ì„¤ëª…
3. "~í–ˆì–´ìš”", "~ë³´ì˜€ì–´ìš”" ë“± ê³¼ê±°í˜•ìœ¼ë¡œ ì‘ì„±
4. ì•„ì´ì˜ ì„ íƒì„ ê¸ì •ì ìœ¼ë¡œ í‰ê°€
"""

        try:
            response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.7
            )

            result = json.loads(response.choices[0].message.content)
            example = result.get("example", f"'{story_title}'ì—ì„œ '{choice_text}'ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
            logger.info(f"ì˜ˆì‹œ ì„¤ëª… ìƒì„± ì™„ë£Œ: {len(example)}ì")
            return {"example": example}

        except Exception as e:
            logger.warning(f"ì˜ˆì‹œ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
            return {"example": f"'{story_title}'ì—ì„œ '{choice_text}'ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤."}

    except Exception as e:
        logger.exception("generate-example-description ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate-all-growth-content")
async def generate_all_growth_content(req: GrowthReportRequest):
    """ëª¨ë“  ì„±ì¥ ë¦¬í¬íŠ¸ AI ì½˜í…ì¸ ë¥¼ í•œ ë²ˆì— ìƒì„± (ì„±ëŠ¥ ìµœì í™”)"""
    logger.info(f"í†µí•© AI ì½˜í…ì¸  ìƒì„± ìš”ì²­: totalStories={req.totalStories}, period={req.period}")

    result = {
        "evaluation": "",
        "recommendations": [],
        "milestones": [],
        "strengthDescriptions": [],
        "growthAreaDescriptions": [],
        "examples": {}
    }

    try:
        if not OpenAIService:
            logger.warning("OpenAIService ì—†ìŒ")
            return result

        llm = OpenAIService()

        # 1. AI ì¢…í•© í‰ê°€
        try:
            before_text = "\n".join([f"- {k}: {v:.0f}ì " for k, v in req.beforeAbilities.items()])
            after_text = "\n".join([f"- {k}: {v:.0f}ì " for k, v in req.afterAbilities.items()])
            changes = []
            for ability, after_score in req.afterAbilities.items():
                before_score = req.beforeAbilities.get(ability, 0)
                change = after_score - before_score
                if abs(change) > 5:
                    changes.append(f"{ability}: {change:+.0f}ì ")
            changes_text = ", ".join(changes) if changes else "ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì "

            # ê°•ì  ì˜ì—­ (ì˜ˆì‹œ í¬í•¨)
            strengths_detail = []
            for s in req.strengths[:3]:  # ìƒìœ„ 3ê°œ
                area = s.get("area", "")
                score = s.get("score", 0)
                examples = s.get("examples", [])
                examples_text = ", ".join(examples[:2]) if examples else ""
                strengths_detail.append(f"{area} ({score:.0f}ì ): {examples_text}")
            strengths_text = "\n- ".join(strengths_detail) if strengths_detail else "ì—†ìŒ"

            # ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ (ì˜ˆì‹œ í¬í•¨)
            growth_detail = []
            for g in req.growthAreas[:3]:  # ìƒìœ„ 3ê°œ
                area = g.get("area", "")
                score = g.get("score", 0)
                examples = g.get("examples", [])
                examples_text = ", ".join(examples[:2]) if examples else ""
                growth_detail.append(f"{area} ({score:.0f}ì ): {examples_text}")
            growth_areas_text = "\n- ".join(growth_detail) if growth_detail else "ì—†ìŒ"

            period_map = {"month": "í•œ ë‹¬", "quarter": "3ê°œì›”", "halfyear": "6ê°œì›”"}
            period_text = period_map.get(req.period, "í•œ ë‹¬")

            eval_prompt = f"""
ë‹¹ì‹ ì€ ì•„ë™ì‹¬ë¦¬ì „ë¬¸ìƒë‹´ê°€ì…ë‹ˆë‹¤. ì•„ì´ì˜ {period_text}ê°„ ì„±ì¥ ë¦¬í¬íŠ¸ë¥¼ ìœ„í•œ ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” ì¢…í•© í‰ê°€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ê¸°ë³¸ ì •ë³´**:
- ì™„ë£Œí•œ ë™í™”: {req.totalStories}ê°œ
- ê¸°ê°„: {period_text}

**ì´ì „ ëŠ¥ë ¥ì¹˜**:
{before_text}

**í˜„ì¬ ëŠ¥ë ¥ì¹˜**:
{after_text}

**ì£¼ìš” ë³€í™”**: {changes_text}

**ê°•ì  ì˜ì—­ (êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨)**:
- {strengths_text}

**ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ (êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨)**:
- {growth_areas_text}

ì¡°ê±´:
1. **ìµœì†Œ 1000ì ì´ìƒ ì‘ì„± (ë§¤ìš° ì¤‘ìš”!)** - 3-4ê°œ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±
2. ê° ëŠ¥ë ¥ì¹˜ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª… (ì˜ˆ: ìš©ê¸° â†’ ìƒˆë¡œìš´ ë„ì „ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•ŠëŠ” ë§ˆìŒ)
3. **ê°•ì  ì˜ì—­ì˜ êµ¬ì²´ì  ì˜ˆì‹œë¥¼ í™œìš©**í•˜ì—¬ ì•„ì´ì˜ ì‹¤ì œ í–‰ë™ì„ ì–¸ê¸‰í•˜ê³  ì¹­ì°¬
4. ì™„ë£Œí•œ ë™í™” ê°œìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ì´ì˜ ë…¸ë ¥ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ì •
5. ê¸ì •ì ì´ê³  ì„±ì¥ ê°€ëŠ¥ì„±ì— ì´ˆì ì„ ë§ì¶˜ ê²©ë ¤
6. ë¶€ëª¨ê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ í•œêµ­ì–´
7. í‰ê°€ë¬¸ë§Œ ì‘ì„± (ì œëª©, ì¸ì‚¬ë§, "~ë“œë¦½ë‹ˆë‹¤" ê°™ì€ ê²°ì–´ ì œì™¸)
8. ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ ì•„ì´ì˜ ì ì¬ë ¥ê³¼ ê°€ëŠ¥ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ **í’ë¶€í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ** ì‘ì„±
9. ê° ë¬¸ë‹¨ ì‚¬ì´ì— ë¹ˆ ì¤„ì„ ë„£ì–´ ê°€ë…ì„± ìˆê²Œ ì‘ì„±
10. **ê° ë¬¸ë‹¨ë§ˆë‹¤ ìµœì†Œ 250ì ì´ìƒ ì‘ì„±í•˜ì—¬ ì „ì²´ 1000ì ì´ìƒ ë‹¬ì„±**

**êµ¬ì¡° (ê° ë¬¸ë‹¨ ìµœì†Œ 250ì ì´ìƒ)**:
- 1ë¬¸ë‹¨: ì „ì²´ì ì¸ ì„±ì¥ ê°œìš”ì™€ ì™„ë£Œí•œ ë™í™”ì— ëŒ€í•œ ì¹­ì°¬ (ì•„ì´ì˜ ë…¸ë ¥ê³¼ ì„±ì¥ì„ í’ë¶€í•˜ê²Œ í‘œí˜„)
- 2ë¬¸ë‹¨: ê°•ì  ì˜ì—­ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ê²©ë ¤ (ì˜ˆì‹œì— ë‚˜ì˜¨ êµ¬ì²´ì  í–‰ë™ì„ ì–¸ê¸‰í•˜ë©° ì¹­ì°¬)
- 3ë¬¸ë‹¨: ì£¼ìš” ë³€í™”ì™€ ë°œì „ ë‚´ìš© (ëŠ¥ë ¥ì¹˜ ë³€í™”ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…)
- 4ë¬¸ë‹¨: ì„±ì¥ ê°€ëŠ¥ ì˜ì—­ê³¼ ì•ìœ¼ë¡œì˜ ê¸°ëŒ€ê° (ë¶€ëª¨ì™€ í•¨ê»˜ í•  ìˆ˜ ìˆëŠ” ë°©í–¥ ì œì‹œ)
"""

            eval_response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": eval_prompt}],
                temperature=0.8,
                max_tokens=2500
            )
            result["evaluation"] = eval_response.choices[0].message.content.strip()
            logger.info(f"ì¢…í•© í‰ê°€ ìƒì„± ì™„ë£Œ: {len(result['evaluation'])}ì")
        except Exception as e:
            logger.error(f"ì¢…í•© í‰ê°€ ìƒì„± ì‹¤íŒ¨: {e}")

        # 2. ì¶”ì²œ í™œë™
        if req.growthAreas:
            try:
                growth_areas_info = "\n".join([
                    f"- {g.get('area', '')}: {g.get('score', 0)}ì  ({g.get('description', '')})"
                    for g in req.growthAreas[:3]
                ])

                rec_prompt = f"""
ì•„ì´ì˜ ì„±ì¥ì„ ìœ„í•œ ë§ì¶¤ í™œë™ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ì„±ì¥ ê°€ëŠ¥ ì˜ì—­**:
{growth_areas_info}

ìœ„ ì˜ì—­ë“¤ì„ ê³ ë ¤í•˜ì—¬ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ìˆœì„œë¡œ 3ê°€ì§€ í™œë™ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "recommendations": [
    {{"priority": 1, "activity": "í™œë™ ì´ë¦„", "description": "êµ¬ì²´ì ì¸ í™œë™ ì„¤ëª… (40ì ì´ë‚´)", "targetArea": "íƒ€ê²Ÿ ëŠ¥ë ¥ì¹˜"}},
    {{"priority": 2, "activity": "í™œë™ ì´ë¦„", "description": "í™œë™ ì„¤ëª…", "targetArea": "íƒ€ê²Ÿ ëŠ¥ë ¥ì¹˜"}},
    {{"priority": 3, "activity": "í™œë™ ì´ë¦„", "description": "í™œë™ ì„¤ëª…", "targetArea": "íƒ€ê²Ÿ ëŠ¥ë ¥ì¹˜"}}
  ]
}}

ì¡°ê±´:
1. ì•„ì´ê°€ ì‹¤ì œë¡œ í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì´ê³  ì¬ë¯¸ìˆëŠ” í™œë™
2. ë¶€ëª¨ê°€ í•¨ê»˜ í•  ìˆ˜ ìˆëŠ” í™œë™
3. ì¼ìƒì—ì„œ ì‰½ê²Œ ì‹¤ì²œ ê°€ëŠ¥
"""

                rec_response = llm.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": rec_prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.7
                )
                rec_data = json.loads(rec_response.choices[0].message.content)
                result["recommendations"] = rec_data.get("recommendations", [])
                logger.info(f"ì¶”ì²œ í™œë™ ìƒì„± ì™„ë£Œ: {len(result['recommendations'])}ê°œ")
            except Exception as e:
                logger.error(f"ì¶”ì²œ í™œë™ ìƒì„± ì‹¤íŒ¨: {e}")

        # 3. ë§ˆì¼ìŠ¤í†¤ (ë™ì‹œ ìƒì„± - íš¨ìœ¨ì„±)
        milestones = []
        if req.totalStories >= 5:
            try:
                ms_prompt = f"""
ì•„ì´ê°€ {req.totalStories}ê°œì˜ ë™í™”ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì¶•í•˜ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
JSON: {{"achievement": "ì¶•í•˜ ë¬¸êµ¬ (20ì ì´ë‚´)"}}
ì¡°ê±´: ë…¸ë ¥ê³¼ ê¾¸ì¤€í•¨ ê°•ì¡°, ê¸ì •ì  ì–´ì¡°
"""
                ms_resp = llm.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": ms_prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.7
                )
                ms_data = json.loads(ms_resp.choices[0].message.content)
                milestones.append({"achievement": ms_data.get("achievement", ""), "date": None})
            except Exception as e:
                logger.error(f"ë™í™” ì™„ë£Œ ë§ˆì¼ìŠ¤í†¤ ì‹¤íŒ¨: {e}")

        for ability, score in req.afterAbilities.items():
            if score >= 75:
                try:
                    ab_prompt = f"""
ì•„ì´ê°€ {ability} ëŠ¥ë ¥ì—ì„œ {score:.0f}ì  ë‹¬ì„±. ì¶•í•˜ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
JSON: {{"achievement": "ì¶•í•˜ ë¬¸êµ¬ (25ì ì´ë‚´, {ability}ì˜ ì˜ë¯¸ ì‰½ê²Œ í’€ì–´ì„œ)"}}
ì¡°ê±´: ëŠ¥ë ¥ì¹˜ ì‰½ê²Œ ì„¤ëª…, ë”°ëœ»í•œ ì–´ì¡°
"""
                    ab_resp = llm.client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": ab_prompt}],
                        response_format={"type": "json_object"},
                        temperature=0.7
                    )
                    ab_data = json.loads(ab_resp.choices[0].message.content)
                    milestones.append({"achievement": ab_data.get("achievement", ""), "date": None})
                except Exception as e:
                    logger.error(f"{ability} ë§ˆì¼ìŠ¤í†¤ ì‹¤íŒ¨: {e}")

        result["milestones"] = milestones
        logger.info(f"ë§ˆì¼ìŠ¤í†¤ ìƒì„± ì™„ë£Œ: {len(milestones)}ê°œ")

        # 4. ê°•ì  ì˜ì—­ ì„¤ëª…
        strength_descs = []
        for strength_info in req.strengths[:3]:
            area_name = strength_info.get("area", "")
            score = strength_info.get("score", 0)
            examples = strength_info.get("examples", [])  # ë°°ì—´ë¡œ ë°›ê¸°

            try:
                st_prompt = f"""
ì•„ì´ê°€ {area_name} ëŠ¥ë ¥ì—ì„œ {score}ì ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. {f"ì˜ˆì‹œ: {', '.join(examples)}" if examples else ""}
ì´ ê°•ì ì„ ë¶€ëª¨ì—ê²Œ ë³´ê³ í•˜ëŠ” ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
JSON: {{"description": "{area_name}ì˜ ì˜ë¯¸ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì•„ì´ì˜ ê°•ì ì„ 3ì¸ì¹­ìœ¼ë¡œ ì„¤ëª… (ì˜ˆ: ì•„ì´ëŠ”, ì•„ì´ì˜) 40ì ì´ë‚´"}}
ì¡°ê±´: ë¶€ëª¨ì—ê²Œ ë³´ê³ í•˜ëŠ” í˜•ì‹, 3ì¸ì¹­ ì‚¬ìš©, êµ¬ì²´ì  ì¹­ì°¬, ë”°ëœ»í•œ ì–´ì¡°
"""
                st_resp = llm.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": st_prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.7
                )
                st_data = json.loads(st_resp.choices[0].message.content)
                strength_descs.append({
                    "area": area_name,
                    "score": score,
                    "description": st_data.get("description", ""),
                    "examples": examples  # ë°°ì—´ë¡œ ë°˜í™˜
                })
            except Exception as e:
                logger.error(f"{area_name} ê°•ì  ì„¤ëª… ì‹¤íŒ¨: {e}")
                strength_descs.append({
                    "area": area_name,
                    "score": score,
                    "description": f"{area_name} ì˜ì—­ì—ì„œ ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
                    "examples": examples  # ë°°ì—´ë¡œ ë°˜í™˜
                })

        result["strengthDescriptions"] = strength_descs
        logger.info(f"ê°•ì  ì„¤ëª… ìƒì„± ì™„ë£Œ: {len(strength_descs)}ê°œ")

        # 5. ì„±ì¥ê°€ëŠ¥ì˜ì—­ ì„¤ëª…
        growth_descs = []
        for area_info in req.growthAreas[:3]:
            area_name = area_info.get("area", "")
            score = area_info.get("score", 0)
            examples = area_info.get("examples", [])

            try:
                # ì˜ˆì‹œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                examples_text = ""
                if examples:
                    examples_text = f"\n\n**ì•„ì´ê°€ ì„ íƒí•œ ì˜ˆì‹œ**\n" + "\n".join([f"- {ex}" for ex in examples[:3]])
                ga_prompt = f"""
ì•„ì´ì˜ {area_name} ëŠ¥ë ¥(í˜„ì¬ {score}ì )ì„ ë°œì „ì‹œí‚¤ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë¶„ì„ê³¼ ì¶”ì²œì„ ì‘ì„±í•´ì£¼ì„¸ìš”.{examples_text}
ë¶€ëª¨ì—ê²Œ ì „ë‹¬í•  ë‚´ìš©ì„ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
{{
  "description": "100-150ì ì´ë‚´ì˜ ìƒì„¸í•œ ì„¤ëª…",
  "recommendation": "60-80ì ì´ë‚´ì˜ êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ í™œë™"
}}

**description ì‘ì„± ê°€ì´ë“œ**:
1. {area_name} ëŠ¥ë ¥ì´ ë¬´ì—‡ì¸ì§€ ì‰½ê²Œ ì„¤ëª… (ì˜ˆ: ìš©ê¸° â†’ ìƒˆë¡œìš´ ê²ƒì— ë„ì „í•˜ëŠ” ë§ˆìŒ)
2. ìœ„ ì˜ˆì‹œë“¤ì„ ì–¸ê¸‰í•˜ë©° ì•„ì´ì˜ í˜„ì¬ ìƒí™©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
   - ì˜ˆì‹œê°€ ìˆìœ¼ë©´: "~ì—ì„œ ~ë¥¼ ì„ íƒí–ˆëŠ”ë°, ì´ëŠ”..."
   - ì˜ˆì‹œê°€ ì—†ìœ¼ë©´: "ì•„ì§ ì´ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤„ ê¸°íšŒê°€ ì ì—ˆìŠµë‹ˆë‹¤."
3. ì™œ ì´ ëŠ¥ë ¥ì´ ì¤‘ìš”í•œì§€ ë¶€ëª¨ ê´€ì ì—ì„œ ì„¤ëª…
4. 3ì¸ì¹­ ì‚¬ìš© (ì˜ˆ: ì•„ì´ëŠ”, ì•„ì´ê°€, ì•„ì´ì˜)

**recommendation ì‘ì„± ê°€ì´ë“œ**:
1. ì¼ìƒì—ì„œ ë°”ë¡œ ì‹¤ì²œ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ í™œë™ ì œì‹œ
2. ë¶€ëª¨ì™€ í•¨ê»˜ í•  ìˆ˜ ìˆëŠ” í™œë™
3. ì•„ì´ì˜ í¥ë¯¸ë¥¼ ëŒ ìˆ˜ ìˆëŠ” ì¬ë¯¸ìˆëŠ” ë°©ë²•
4. "~í•´ë³´ì„¸ìš”", "~í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤" ë“± ì œì•ˆí•˜ëŠ” ì–´ì¡°
5. {area_name} ê´€ë ¨ ë™í™” ì½ê¸°ë¥¼ í¬í•¨í•˜ë˜, ì¶”ê°€ ì•¡ì…˜ ì•„ì´í…œë„ ì œì‹œ

**ì–´ì¡°**: ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ”, ì „ë¬¸ê°€ê°€ ë¶€ëª¨ì—ê²Œ ì¡°ì–¸í•˜ëŠ” í†¤
"""
                ga_resp = llm.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": ga_prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.7,
                    max_tokens=500  # ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
                )
                ga_data = json.loads(ga_resp.choices[0].message.content)
                growth_descs.append({
                    "area": area_name,
                    "score": score,
                    "description": ga_data.get("description", ""),
                    "recommendation": ga_data.get("recommendation", ""),
                    "examples": examples  # ì˜ˆì‹œë„ ë°˜í™˜
                })

            except Exception as e:
                logger.error(f"{area_name} ì„±ì¥ì˜ì—­ ì„¤ëª… ì‹¤íŒ¨: {e}")
                growth_descs.append({
                    "area": area_name,
                    "score": score,
                    "description": f"{area_name} ì˜ì—­ì„ ë” ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "recommendation": f"{area_name} ê´€ë ¨ ë™í™”ë¥¼ í•¨ê»˜ ì½ì–´ë³´ì„¸ìš”.",
                    "examples": examples
                })
        
        result ["growthAreaDescriptions"] = growth_descs
        logger.info(f"ì„±ì¥ì˜ì—­ ì„¤ëª… ìƒì„± ì™„ë£Œ: {len(growth_descs)}ê°œ")

        logger.info("í†µí•© AI ì½˜í…ì¸  ìƒì„± ì™„ë£Œ")
        return result
    
    except Exception as e:
        logger.exception("generate-all-growth-content ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/analyze-choice-pattern")
async def analyze_choice_pattern(request: Dict[str, Any]):
    """ì•„ì´ì˜ ì„ íƒ íŒ¨í„´ì„ AIë¡œ ë¶„ì„í•˜ì—¬ ìŠ¤íƒ€ì¼ ë¶„ë¥˜"""
    logger.info("ì„ íƒ íŒ¨í„´ ë¶„ì„ ìš”ì²­")
    try:
        ability_type = request.get("abilityType", "")
        ability_ratios = request.get("abilityRatios", {})

        if not OpenAIService or not ability_type:
            # í´ë°±: ê¸°ë³¸ ìŠ¤íƒ€ì¼
            default_styles = {
                "ìš©ê¸°": "ìš©ê°í•œ ì„ íƒ",
                "ì¹œì ˆ": "ë°°ë ¤í•˜ëŠ” ì„ íƒ",
                "ê³µê°": "ë°°ë ¤í•˜ëŠ” ì„ íƒ",
                "ìš°ì •": "í˜‘ë ¥í•˜ëŠ” ì„ íƒ",
                "ìì¡´ê°": "ìì‹ ìˆëŠ” ì„ íƒ"
            }
            return {"style": default_styles.get(ability_type, "ìš©ê°í•œ ì„ íƒ")}

        llm = OpenAIService()

        # ë¹„ìœ¨ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        ratios_text = ", ".join([f"{k}: {v:.1f}%" for k, v in ability_ratios.items()])

        prompt = f"""
ì•„ì´ì˜ ì„ íƒ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ëŠ¥ë ¥ì¹˜ ë¶„í¬**:
{ratios_text}

**í˜„ì¬ ì„ íƒí•œ ëŠ¥ë ¥ì¹˜**: {ability_type}

ì•„ì´ì˜ ì „ì²´ì ì¸ ì„ íƒ íŒ¨í„´ì„ ë³´ê³ , ì´ ì„ íƒì´ ì–´ë–¤ ìŠ¤íƒ€ì¼ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "style": "ì„ íƒ ìŠ¤íƒ€ì¼ (ì•„ë˜ 6ê°€ì§€ ì¤‘ 1ê°œ)"
}}

**ê°€ëŠ¥í•œ ìŠ¤íƒ€ì¼**:
1. "ìš©ê°í•œ ì„ íƒ" - ìš©ê¸°ë¥¼ í¬ê²Œ ë³´ì´ë©° ê³¼ê°í•˜ê²Œ ë„ì „
2. "ë°°ë ¤í•˜ëŠ” ì„ íƒ" - íƒ€ì¸ì„ ìƒê°í•˜ëŠ” ë”°ëœ»í•œ ë§ˆìŒ
3. "í˜‘ë ¥í•˜ëŠ” ì„ íƒ" - í•¨ê»˜í•˜ëŠ” ê²ƒì„ ì¤‘ìš”ì‹œ
4. "ìì‹ ìˆëŠ” ì„ íƒ" - ìì¡´ê°ê³¼ í™•ì‹ ì„ ê°€ì§€ê³  í–‰ë™
5. "ë„ì „ì ì¸ ì„ íƒ" - ìƒˆë¡œìš´ ê²ƒì— ë„ì „í•˜ëŠ” ëª¨ìŠµ
6. "ì‹ ì¤‘í•œ ì„ íƒ" - ê¹Šì´ ìƒê°í•˜ê³  íŒë‹¨

ì¡°ê±´:
1. ëŠ¥ë ¥ì¹˜ ë¶„í¬ì™€ ì„ íƒí•œ ëŠ¥ë ¥ì¹˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤
2. ìœ„ 6ê°€ì§€ ìŠ¤íƒ€ì¼ ì¤‘ ì •í™•íˆ í•˜ë‚˜ë§Œ ì„ íƒ
3. ëŠ¥ë ¥ì¹˜ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³  ì˜ë¯¸ë¥¼ í•´ì„
"""

        try:
            response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.7
            )

            result = json.loads(response.choices[0].message.content)
            style = result.get("style", "ìš©ê°í•œ ì„ íƒ")

            # ìœ íš¨í•œ ìŠ¤íƒ€ì¼ì¸ì§€ ê²€ì¦
            valid_styles = ["ìš©ê°í•œ ì„ íƒ", "ë°°ë ¤í•˜ëŠ” ì„ íƒ", "í˜‘ë ¥í•˜ëŠ” ì„ íƒ",
                           "ìì‹ ìˆëŠ” ì„ íƒ", "ë„ì „ì ì¸ ì„ íƒ", "ì‹ ì¤‘í•œ ì„ íƒ"]
            if style not in valid_styles:
                style = "ìš©ê°í•œ ì„ íƒ"

            logger.info(f"ì„ íƒ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: {ability_type} â†’ {style}")
            return {"style": style}

        except Exception as e:
            logger.warning(f"AI ì„ íƒ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í´ë°±
            default_styles = {
                "ìš©ê¸°": "ìš©ê°í•œ ì„ íƒ",
                "ì¹œì ˆ": "ë°°ë ¤í•˜ëŠ” ì„ íƒ",
                "ê³µê°": "ë°°ë ¤í•˜ëŠ” ì„ íƒ",
                "ìš°ì •": "í˜‘ë ¥í•˜ëŠ” ì„ íƒ",
                "ìì¡´ê°": "ìì‹ ìˆëŠ” ì„ íƒ"
            }
            return {"style": default_styles.get(ability_type, "ìš©ê°í•œ ì„ íƒ")}

    except Exception as e:
        logger.exception("analyze-choice-pattern ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/analyze-chat-pattern")
async def analyze_chat_pattern(request: Dict[str, Any]):
    """ëŒ€í™” íŒ¨í„´ AI ë¶„ì„"""
    logger.info("ëŒ€í™” íŒ¨í„´ ë¶„ì„ ìš”ì²­")
    try:
        messages = request.get("messages", [])
        child_name = request.get("childName", "ì•„ì´")

        if not OpenAIService or not messages:
            logger.warning("OpenAIService ì—†ê±°ë‚˜ ë©”ì‹œì§€ ì—†ìŒ")
            return {
                "conversationStyle": "í™œë°œí•œ ëŒ€í™”",
                "vocabularyLevel": "ë‚˜ì´ì— ì ì ˆí•œ ì–´íœ˜ ì‚¬ìš©",
                "mainInterests": [],
                "emotionPattern": "ê¸ì •ì ì¸ ê°ì • í‘œí˜„",
                "participationLevel": "ì ê·¹ì ì¸ ì°¸ì—¬",
                "insights": "ì•„ì´ê°€ ëŒ€í™”ì— ì˜ ì°¸ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤."
            }

        llm = OpenAIService()

        # ì•„ì´ì˜ ë©”ì‹œì§€ë§Œ ì¶”ì¶œ
        child_messages = [msg.get("message", "") for msg in messages if msg.get("sender") == "CHILD"]

        if not child_messages:
            return {
                "conversationStyle": "ëŒ€í™” ì‹œì‘ ë‹¨ê³„",
                "vocabularyLevel": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
                "mainInterests": [],
                "emotionPattern": "ë¶„ì„ ë°ì´í„° ë¶€ì¡±",
                "participationLevel": "ëŒ€í™” ì‹œì‘",
                "insights": "ì•„ì§ ëŒ€í™”ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ íŒ¨í„´ì„ ë¶„ì„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
            }

        # ëŒ€í™” ë‚´ìš© í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        conversation_text = "\n".join([f"- {msg}" for msg in child_messages[:20]])  # ìµœê·¼ 20ê°œ
        message_count = len(child_messages)
        avg_length = sum(len(msg) for msg in child_messages) / len(child_messages) if child_messages else 0

        prompt = f"""
{child_name}ì˜ ì±—ë´‡ ëŒ€í™” íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ëŒ€í™” ë©”ì‹œì§€ ({message_count}ê°œ)**:
{conversation_text}

**í†µê³„**:
- ì´ ë©”ì‹œì§€ ìˆ˜: {message_count}ê°œ
- í‰ê·  ë©”ì‹œì§€ ê¸¸ì´: {avg_length:.1f}ì

ì•„ë™ì‹¬ë¦¬ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ëŒ€í™” íŒ¨í„´ì„ ë¶„ì„í•˜ê³ , ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{{
  "conversationStyle": "ëŒ€í™” ìŠ¤íƒ€ì¼ (20ì ì´ë‚´)",
  "vocabularyLevel": "ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ (30ì ì´ë‚´)",
  "mainInterests": ["ê´€ì‹¬ì‚¬1", "ê´€ì‹¬ì‚¬2", "ê´€ì‹¬ì‚¬3"],
  "emotionPattern": "ê°ì • í‘œí˜„ íŒ¨í„´ (30ì ì´ë‚´)",
  "participationLevel": "ì°¸ì—¬ë„ í‰ê°€ (20ì ì´ë‚´)",
  "insights": "ë¶€ëª¨ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸ (50ì ì´ë‚´)"
}}

**ëŒ€í™” ìŠ¤íƒ€ì¼ ì˜ˆì‹œ**: "í˜¸ê¸°ì‹¬ ë§ê³  ì§ˆë¬¸ì´ ë§ì€ íƒêµ¬í˜•", "ê°ì • í‘œí˜„ì´ í’ë¶€í•œ ê°ì„±í˜•", "ì§§ê³  ëª…í™•í•œ ì‹¤ìš©í˜•" ë“±

**ì–´íœ˜ ìˆ˜ì¤€**: ì•„ì´ì˜ ì—°ë ¹ëŒ€ë¥¼ ê³ ë ¤í•˜ì—¬ í‰ê°€

**ê´€ì‹¬ì‚¬**: ëŒ€í™”ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ì£¼ì œë‚˜ í‚¤ì›Œë“œ (ìµœëŒ€ 3ê°œ)

**ê°ì • íŒ¨í„´**: "ê¸ì •ì  ê°ì • ìœ„ì£¼", "ë‹¤ì–‘í•œ ê°ì • í‘œí˜„", "ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ ê°ì • í‘œí˜„" ë“±

**ì°¸ì—¬ë„**: "ë§¤ìš° ì ê·¹ì ", "ì ê·¹ì ", "ë³´í†µ", "ì†Œê·¹ì " ë“±

**ì¸ì‚¬ì´íŠ¸**: ë¶€ëª¨ê°€ ì•„ì´ì™€ ëŒ€í™”í•  ë•Œ ë„ì›€ì´ ë  ë§Œí•œ ì¡°ì–¸
"""

        try:
            response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.7
            )

            result = json.loads(response.choices[0].message.content)
            logger.info(f"ëŒ€í™” íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: style={result.get('conversationStyle')}")
            return result

        except Exception as e:
            logger.error(f"AI ëŒ€í™” íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # í´ë°±
            return {
                "conversationStyle": "í™œë°œí•œ ëŒ€í™”",
                "vocabularyLevel": "ë‚˜ì´ì— ì ì ˆí•œ ì–´íœ˜ ì‚¬ìš©",
                "mainInterests": ["ë™í™”", "ì¹œêµ¬", "ë†€ì´"],
                "emotionPattern": "ê¸ì •ì ì¸ ê°ì • í‘œí˜„",
                "participationLevel": "ì ê·¹ì ì¸ ì°¸ì—¬",
                "insights": f"{child_name}ê°€ ëŒ€í™”ì— ì˜ ì°¸ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤."
            }

    except Exception as e:
        logger.exception("analyze-chat-pattern ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/extract-chat-topics")
async def extract_chat_topics(request: Dict[str, Any]):
    """
    ëŒ€í™” ë©”ì„¸ì§€ì—ì„œ ì£¼ìš” ì£¼ì œ í‚¤ì›Œë“œ ì¶”ì¶œ + ì‹¬ë¦¬ ë¶„ì„
    """
    logger.info("ëŒ€í™” ì£¼ì œ ì¶”ì¶œ ìš”ì²­")

    messages = request.get("messages", [])

    if not messages:
        logger.warning("ë©”ì„¸ì§€ ì—†ìŒ")
        return {"topics": [], "psychologicalAnalysis": ""}
    
    # ëŒ€í™” ë‚´ìš© ê²°í•©
    conversation_text = "\n".join([
        f"{msg.get('sender', 'unknown')}: {msg.get('message', '')}"
        for msg in messages
    ])

    try:

        llm = OpenAIService()

        # 1. ì£¼ì œ í‚¤ì›Œë“œ ì¶”ì¶œ
        topic_prompt = f"""
ë‹¤ìŒì€ ì•„ì´ì™€ ì±—ë´‡ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_text}

ìœ„ ëŒ€í™”ì—ì„œ ì•„ì´ê°€ ì£¼ë¡œ ê´€ì‹¬ì„ ë³´ì¸ ì£¼ì œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ JSONë§Œ):
{{
  "topics": [
    {{"text": "í‚¤ì›Œë“œ1", "count": ë¹ˆë„ìˆ˜}},
    {{"text": "í‚¤ì›Œë“œ2", "count": ë¹ˆë„ìˆ˜}},
    {{"text": "í‚¤ì›Œë“œ3", "count": ë¹ˆë„ìˆ˜}}
  ]
}}

ì¡°ê±´:
- 5-10ê°œì˜ í‚¤ì›Œë“œ
- ê° í‚¤ì›Œë“œì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ í¬í•¨ (1-10 ì‚¬ì´ì˜ ìˆ«ì)
- ì•„ì´ê°€ ì‹¤ì œë¡œ ì–¸ê¸‰í•œ ì£¼ì œë§Œ í¬í•¨
"""

        topic_response = llm.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": topic_prompt}],
            response_format={"type": "json_object"},
            temperature=0.3,
            max_tokens=500
        )
        topics_text = topic_response.choices[0].message.content.strip()
        logger.info(f"Topics ì›ë³¸ ì‘ë‹µ: {topics_text}")

        topic_data = json.loads(topics_text)
        topics = topic_data.get("topics", [])

        # 2. ì‹¬ë¦¬ ë¶„ì„
        psych_prompt = f"""
ë‹¤ìŒì€ ì•„ì´ì™€ ì±—ë´‡ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_text}

ìœ„ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ì´ì˜ ì‹¬ë¦¬ ìƒíƒœì™€ ê´€ì‹¬ì‚¬ë¥¼ ê°„ë‹¨íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ í¬ì¸íŠ¸:
- ì•„ì´ê°€ ì£¼ë¡œ ê´€ì‹¬ì„ ë³´ì´ëŠ” ì£¼ì œ
- ëŒ€í™”ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ê°ì • ìƒíƒœ
- ê¸ì •ì ì¸ ì¸¡ë©´ê³¼ ë¶€ì •ì ì¸ ì¸¡ë©´ ë°˜ë“œì‹œ í¬í•¨
- ë¶€ëª¨ê°€ ì£¼ëª©í•´ì•¼ í•  ì  (ìˆë‹¤ë©´)

3~4ë¬¸ì¥ìœ¼ë¡œ ë¶€ëª¨ë‹˜ê»˜ ì „ë‹¬í•  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ê°ê´€ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        psych_response = llm.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": psych_prompt}],
            temperature=0.7,
            max_tokens=300
        )
        psychological_analysis = psych_response.choices[0].message.content.strip()

        return {
            "topics": topics,
            "psychologicalAnalysis": psychological_analysis
        }
    
    except Exception as e:
        logger.error(f"ì£¼ì œ ì¶”ì¶œ ë° ì‹¬ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "topics": [],
            "psychologicalAnalysis": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
        
    

@router.post("/generate-dashboard-insights")
async def generate_dashboard_insights(request: Dict[str, Any]):
    """
    ëŒ€ì‹œë³´ë“œ AI ì¸ì‚¬ì´íŠ¸ ìƒì„± (2ê°œ)
    1. Quick ì¸ì‚¬ì´íŠ¸ (ì¢…í•© í˜„í™© íƒ­)
    2. ì¶”ì²œ í™œë™ (ëŠ¥ë ¥ ë°œë‹¬ íƒ­)
    """
    try:
        abilities = request.get("abilities", {})
        choices = request.get("choices", [])
        total_stories = request.get("totalStories", 0)
        period = request.get("period", "week")

        if not OpenAIService:
            return {
                "quickInsight": "ì•„ì´ì™€ í•¨ê»˜ ë™í™”ë¥¼ ì½ìœ¼ë©° ì„±ì¥í•´ë³´ì„¸ìš”!",
                "recommendation": {
                    "ability": "ìš©ê¸°",
                    "message": "ìš©ê¸° ê´€ë ¨ ë™í™”ë¥¼ í•¨ê»˜ ì½ì–´ë³´ì„¸ìš”."
                }
            }

        llm = OpenAIService()

        # 1. Quick ì¸ì‚¬ì´íŠ¸ ìƒì„±
        top_ability = max(abilities.items(), key=lambda x: x[1]) if abilities else None
        low_ability = min(abilities.items(), key=lambda x: x[1]) if abilities else None
        top_choice = choices[0] if choices else None

        logger.info(f"ğŸ“Š Quick ì¸ì‚¬ì´íŠ¸ ì…ë ¥ ë°ì´í„°: top_ability={top_ability}, top_choice={top_choice}")

        period_text = {"day": "ì˜¤ëŠ˜", "week": "ì´ë²ˆ ì£¼", "month": "ì´ë²ˆ ë‹¬"}.get(period, "ì´ë²ˆ ì£¼")

        quick_prompt = f"""
ì•„ì´ì˜ {period_text} í™œë™ ë°ì´í„°ë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤:
- ì™„ë£Œí•œ ë™í™”: {total_stories}ê°œ
- ê°€ì¥ ë†’ì€ ëŠ¥ë ¥: {top_ability[0]} ({top_ability[1]:.0f}ì ) (ìµœê³ ì¸ ëŠ¥ë ¥)
- ê°€ì¥ ë‚®ì€ ëŠ¥ë ¥: {low_ability[0]} ({low_ability[1]:.0f}ì ) (ê°œì„ ì´ í•„ìš”í•œ ëŠ¥ë ¥)
- **ì£¼ìš” ì„ íƒ ìŠ¤íƒ€ì¼: {top_choice['name']} ({top_choice['value']}%)**

ë¶€ëª¨ì—ê²Œ ì „ë‹¬í•  ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” í•œ ì¤„ ì¸ì‚¬ì´íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì¡°ê±´:
1. 40ì ì´ë‚´
2. ì•„ì´ì˜ ê°•ì ì„ ì¹­ì°¬í•˜ê³ , ê°œì„ ì ì„ ë¶€ë“œëŸ½ê²Œ ì œì•ˆ
3. êµ¬ì²´ì ì¸ ëŠ¥ë ¥ëª…ê³¼ ìˆ˜ì¹˜ ì–¸ê¸‰
4. **ë°˜ë“œì‹œ ì£¼ìš” ì„ íƒ ìŠ¤íƒ€ì¼ "{top_choice['name']}"ì„ ì–¸ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤**
5. "~í•´ìš”", "~ë³´ì„¸ìš”" ë“± ì¹œê·¼í•œ ì–´ì¡°

ì˜ˆì‹œ:
- "ìš©ê¸°ê°€ ë†’ê³  ìš©ê°í•œ ì„ íƒì„ ì£¼ë¡œ í•˜ê³  ìˆì–´ìš”!"
- "ë°°ë ¤í•˜ëŠ” ì„ íƒì´ ë§ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ìš”!"

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "insight": "í•œ ì¤„ ì¸ì‚¬ì´íŠ¸"
}}
"""

        try:
            quick_response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": quick_prompt}],
                response_format={"type": "json_object"},
                temperature=0.8
            )
            quick_data = json.loads(quick_response.choices[0].message.content)
            quick_insight = quick_data.get("insight", "ì•„ì´ì™€ í•¨ê»˜ ë™í™”ë¥¼ ì½ìœ¼ë©° ì„±ì¥í•´ë³´ì„¸ìš”!")
            logger.info(f"âœ… Quick ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {quick_insight}")
        except Exception as e:
            logger.error(f"Quick ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            quick_insight = f"{top_ability[0] if top_ability else 'ëŠ¥ë ¥'}ì´ ë†’ê³ , {top_choice['name'] if top_choice else 'ì¢‹ì€ ì„ íƒ'}ì„ ì£¼ë¡œ í•˜ê³  ìˆì–´ìš”."

        # 2. ëŠ¥ë ¥ ì¶”ì²œ í™œë™ ìƒì„±
        rec_prompt = f"""
ì•„ì´ì˜ ëŠ¥ë ¥ì¹˜ ë¶„ì„ ê²°ê³¼:
- ê°€ì¥ ë‚®ì€ ëŠ¥ë ¥: {low_ability[0]} ({low_ability[1]:.0f}ì )

ì´ ëŠ¥ë ¥ì„ í‚¤ìš¸ ìˆ˜ ìˆëŠ” ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì¡°ê±´:
1. 30ì ì´ë‚´
2. {low_ability[0]} ëŠ¥ë ¥ì„ í‚¤ìš°ëŠ” êµ¬ì²´ì ì¸ í™œë™ ì œì•ˆ
3. "~í•´ë³´ì„¸ìš”", "~ëŠ” ì–´ë–¨ê¹Œìš”?" ë“± ì œì•ˆí•˜ëŠ” ì–´ì¡°

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "message": "ì¶”ì²œ ë©”ì‹œì§€"
}}
"""

        try:
            rec_response = llm.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": rec_prompt}],
                response_format={"type": "json_object"},
                temperature=0.8
            )
            rec_data = json.loads(rec_response.choices[0].message.content)
            rec_message = rec_data.get("message", f"{low_ability[0] if low_ability else 'ëŠ¥ë ¥'} ê´€ë ¨ ë™í™”ë¥¼ í•¨ê»˜ ì½ì–´ë³´ì„¸ìš”.")
        except Exception as e:
            logger.error(f"ì¶”ì²œ í™œë™ ìƒì„± ì‹¤íŒ¨: {e}")
            rec_message = f"{low_ability[0] if low_ability else 'ëŠ¥ë ¥'} ê´€ë ¨ ë™í™”ë¥¼ í•¨ê»˜ ì½ìœ¼ë©´ì„œ í‚¤ì›Œë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"

        logger.info("ëŒ€ì‹œë³´ë“œ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ")
        return {
            "quickInsight": quick_insight,
            "recommendation": {
                "ability": low_ability[0] if low_ability else "ìš©ê¸°",
                "message": rec_message
            }
        }

    except Exception as e:
        logger.exception("generate-dashboard-insights ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/health-growth")
async def health_growth():
    """ì„±ì¥ ë¦¬í¬íŠ¸ API í—¬ìŠ¤ì²´í¬"""
    logger.info("health check ìš”ì²­ (ì„±ì¥ ë¦¬í¬íŠ¸)")
    return {"status": "ok", "service": "growth_report"}